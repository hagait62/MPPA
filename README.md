## Multi Pairwise Procrustes Analysis
This repo contains the source code for our paper:

[**A Multi-Pairwise Extension of Procrustes Analysis for Multilingual Word Translation**](https://www.aclweb.org/anthology/D19-1363.pdf)
<br>
Hagai Taitelbaum,
[Gal Chechik](https://chechiklab.biu.ac.il/~gal/),
[Jacob Goldberger](http://www.eng.biu.ac.il/goldbej/)
<br>
EMNLP 2019
<br>
[bibtex](https://www.aclweb.org/anthology/D19-1363.bib)

(built over MUSE implementation, https://github.com/facebookresearch/MUSE)

## Environment
For installing the appropriate environment with conda, run the following commands (with anaconda3):
```
conda create --name mppa_env python=3.6 --yes
conda activate mppa_env
conda config --append channels conda-forge
conda config --append channels pytorch
conda install --yes --file requirements.txt
```

## Dependencies
* Python 3.6 with [NumPy](http://www.numpy.org/)/[SciPy](https://www.scipy.org/)
* [PyTorch](http://pytorch.org/)
* [Faiss](https://github.com/facebookresearch/faiss) (recommended) for fast nearest neighbor search (CPU or GPU).

Faiss is *optional* for GPU users - though Faiss-GPU will greatly speed up nearest neighbor search - and *highly recommended* for CPU users. Faiss can be installed using "conda install faiss-cpu -c pytorch" or "conda install faiss-gpu -c pytorch".

## Get monolingual word embeddings
For pre-trained monolingual word embeddings, we highly recommend [fastText Wikipedia embeddings](https://fasttext.cc/docs/en/pretrained-vectors.html), or using [fastText](https://github.com/facebookresearch/fastText) to train your own word embeddings from your corpus.

You can download the embeddings of several languages by running:
```bash
sh get_monolingual_embeddings.sh "LANGS" PATH
```
where LANGS is all the desired languages (space-seperated),
and PATH is the directory to save the embeddings in (optional, default is ./data/vecs/).
For example:
```bash
sh get_monolingual_embeddings.sh "en de fr es it pt" ./data/vecs/
```
### Word embedding format

When loading embeddings, the model can load:
* PyTorch binary files previously generated by MUSE (.pth files).
* fastText binary files previously generated by fastText (.bin files).
* text files (text file with one word embedding per line).

The two first options are very fast and can load 1 million embeddings in a few seconds, while loading text files can take a while.

## Get evaluation dictionaries
Download evaluation dictionaries for several languages:
```bash
sh get_evaluation_dictionaries.sh "LANGS" PATH
```
where LANGS is all the desired languages (space-seperated),
and PATH is the directory to save the dictionaries in (optional, default is ./data/dictionaries/).
All available permutations will be downloaded.

The evaluation datasets are also available at:
* MUSE [bilingual dictionaries](https://github.com/facebookresearch/MUSE#ground-truth-bilingual-dictionaries)

## Apply MPPA
To obtain MPPA results for the **six-european languages experiment**, simply run:
```bash
python3 supervised.py --langs en de fr es it pt --embs DATA/VECS/wiki.en.vec DATA/VECS/wiki.de.vec
DATA/VECS/wiki.fr.vec DATA/VECS/wiki.es.vec DATA/VECS/wiki.it.vec DATA/VECS/wiki.pt.vec --dico_train MAT
--dicts_train_path ./data/MAT_extracted_dictionaries --dicts_eval_path DICTS_PATH --n_refinement 5
```
DATA/VECS/ is the directory which contains the embeddings,
and DICTS_PATH is the directory which contains the evaluation dictionaries.

For the **language triplets experiment** (for example: English, German and Afrikaans), simply run:
```bash
python3 supervised.py --langs en de af --embs DATA/VECS/wiki.en.vec DATA/VECS/wiki.de.vec
DATA/VECS/wiki.af.vec --dico_train identical_char --dicts_eval_path DICTS_PATH --n_refinement 10
```

By default we re-norm and center the embeddings. To change it use `--normalize_embeddings=`.

### MAT bilingual dictionaries

For MAT+MPPA we ran MAT and extracted the dictionaries after the MAT step, using [MAT+MPSR source code](https://github.com/ccsasuke/umwe).
Then we used these dictionaries as the supervision for MPPA.
We provide the dictionaries under `./data/MAT_extracted_dictionaries`.